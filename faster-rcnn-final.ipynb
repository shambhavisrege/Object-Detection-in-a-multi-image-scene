{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.utils.data\n",
    "from torch.utils.data import SubsetRandomSampler\n",
    "from torch.optim import lr_scheduler\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torchvision import datasets, models\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import torchnet as tnt\n",
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import yaml\n",
    "from engine import train_one_epoch, evaluate\n",
    "plt.ion()\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "\n",
    "   \n",
    "class Args():\n",
    "    \n",
    "    workers = 4\n",
    "    batchSize = 64\n",
    "    niter = 25\n",
    "    lr = 0.001\n",
    "    cuda = \"cuda:0\"\n",
    "    ngpu = 3\n",
    "    outf = '.'\n",
    "    \n",
    "class LabeledDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, split, transforms): \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root: Location of the dataset folder, usually it is /labeled\n",
    "            split: The split you want to used, it should be training or validation\n",
    "            transform: the transform you want to applied to the images.\n",
    "        \"\"\"\n",
    "\n",
    "        self.split = split\n",
    "        self.transforms = transforms\n",
    "\n",
    "        self.image_dir = os.path.join(root,split, \"images\") \n",
    "        self.label_dir = os.path.join(root,split, \"labels\") \n",
    "\n",
    "        self.num_images = len(os.listdir(self.image_dir))\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_images  \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "#         the idx of training image is from 1 to 30000\n",
    "#         the idx of validation image is from 30001 to 50000\n",
    "\n",
    "        if self.split == \"training\":\n",
    "            offset = 1\n",
    "        if self.split == \"validation\":\n",
    "            offset = 30001\n",
    "\n",
    "        with open(os.path.join(self.image_dir, f\"{offset + idx}.JPEG\"), \"rb\") as f:\n",
    "            img = Image.open(f).convert(\"RGB\")\n",
    "        with open(os.path.join(self.label_dir, f\"{offset + idx}.yml\"), \"rb\") as f:\n",
    "            yamlfile = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "        num_objs = len(yamlfile[\"labels\"])\n",
    "        # xmin, ymin, xmax, ymax\n",
    "        boxes = torch.as_tensor(yamlfile[\"bboxes\"], dtype=torch.float32)\n",
    "        labels = []\n",
    "        for label in yamlfile[\"labels\"]:\n",
    "            labels.append(class_dict[label])\n",
    "        labels = torch.as_tensor(labels, dtype=torch.int64)\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
    "\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "\n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "\n",
    "        return img, target\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "def do_training(model, torch_dataset, torch_dataset_test, num_epochs=10):\n",
    "    # define training and validation data loaders\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        torch_dataset, batch_size= 16, shuffle=True, num_workers=4,\n",
    "        collate_fn=collate_fn)\n",
    "    \n",
    "    data_loader_test = torch.utils.data.DataLoader(\n",
    "        torch_dataset_test, batch_size= 16, shuffle=False, num_workers=4,\n",
    "        collate_fn=collate_fn)\n",
    "\n",
    "    # train on the GPU or on the CPU, if a GPU is not available\n",
    "    device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "    print(\"Using device %s\" % device)\n",
    "\n",
    "    # move model to the right device\n",
    "    model.to(device)\n",
    "\n",
    "    # construct an optimizer\n",
    "    params = [p for p in model.parameters() if p.requires_grad]\n",
    "    optimizer = torch.optim.Adam(params, lr=0.001)\n",
    "    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n",
    "                                                    step_size=3,\n",
    "                                                    gamma=0.1)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=15)\n",
    "        lr_scheduler.step()\n",
    "        evaluate(model, data_loader_test, device=device)\n",
    "        torch.save(model.state_dict(), '%s/faster_rcnn_epoch.pth' % (opt.outf))\n",
    "    return model\n",
    "\n",
    "class_dict = {\n",
    "    \"cup or mug\": 0, \"bird\": 1, \"hat with a wide brim\": 2, \"person\": 3, \"dog\": 4, \"lizard\": 5, \"sheep\": 6, \"wine bottle\": 7,\n",
    "    \"bowl\": 8, \"airplane\": 9, \"domestic cat\": 10, \"car\": 11, \"porcupine\": 12, \"bear\": 13, \"tape player\": 14, \"ray\": 15, \"laptop\": 16,\n",
    "    \"zebra\": 17, \"computer keyboard\": 18, \"pitcher\": 19, \"artichoke\": 20, \"tv or monitor\": 21, \"table\": 22, \"chair\": 23,\n",
    "    \"helmet\": 24, \"traffic light\": 25, \"red panda\": 26, \"sunglasses\": 27, \"lamp\": 28, \"bicycle\": 29, \"backpack\": 30, \"mushroom\": 31,\n",
    "    \"fox\": 32, \"otter\": 33, \"guitar\": 34, \"microphone\": 35, \"strawberry\": 36, \"stove\": 37, \"violin\": 38, \"bookshelf\": 39,\n",
    "    \"sofa\": 40, \"bell pepper\": 41, \"bagel\": 42, \"lemon\": 43, \"orange\": 44, \"bench\": 45, \"piano\": 46, \"flower pot\": 47, \"butterfly\": 48,\n",
    "    \"purse\": 49, \"pomegranate\": 50, \"train\": 51, \"drum\": 52, \"hippopotamus\": 53, \"ski\": 54, \"ladybug\": 55, \"banana\": 56, \"monkey\": 57,\n",
    "    \"bus\": 58, \"miniskirt\": 59, \"camel\": 60, \"cream\": 61, \"lobster\": 62, \"seal\": 63, \"horse\": 64, \"cart\": 65, \"elephant\": 66,\n",
    "    \"snake\": 67, \"fig\": 68, \"watercraft\": 69, \"apple\": 70, \"antelope\": 71, \"cattle\": 72, \"whale\": 73, \"coffee maker\": 74, \"baby bed\": 75,\n",
    "    \"frog\": 76, \"bathing cap\": 77, \"crutch\": 78, \"koala bear\": 79, \"tie\": 80, \"dumbbell\": 81, \"tiger\": 82, \"dragonfly\": 83, \"goldfish\": 84,\n",
    "    \"cucumber\": 85, \"turtle\": 86, \"harp\": 87, \"jellyfish\": 88, \"swine\": 89, \"pretzel\": 90, \"motorcycle\": 91, \"beaker\": 92, \"rabbit\": 93,\n",
    "    \"nail\": 94, \"axe\": 95, \"salt or pepper shaker\": 96, \"croquet ball\": 97, \"skunk\": 98, \"starfish\": 99,\n",
    "}\n",
    "\n",
    "def get_model(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(pretrained=False)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    VALID_DATASET_PATH = \"/scratch/sr6172/DL/labeled_data/labeled_data\"\n",
    "    \n",
    "    train_dataset = LabeledDataset(\n",
    "        root=VALID_DATASET_PATH,\n",
    "        split=\"training\",\n",
    "        transforms=  lambda x, y: (torchvision.transforms.functional.to_tensor(x), y),\n",
    "    )\n",
    "    \n",
    "    valid_dataset = LabeledDataset(\n",
    "        root=VALID_DATASET_PATH,\n",
    "        split=\"validation\",\n",
    "        transforms=  lambda x, y: (torchvision.transforms.functional.to_tensor(x), y),\n",
    "    )\n",
    "    opt = Args()\n",
    "    f = open(\"{}/training_logs_faster_rcnn_10_epochs_final.txt\".format(opt.outf),\"w+\")\n",
    "    device = torch.device(\"cuda:0\" if opt.cuda else \"cpu\")\n",
    "    f.write(\"using \" + str(device) + \"\\n\")\n",
    "    f.flush()\n",
    "    model_ft = nn.DataParallel(get_model(100))\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    model = do_training(model_ft, train_dataset, valid_dataset, num_epochs=10)\n",
    "    torch.save(model.state_dict(), '%s/faster_rcnn_10_epochs_final.pth' % (opt.outf))\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, '%s/MOBILENET.pt' % (opt.outf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
